{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/andrewlb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/andrewlb/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/andrewlb/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, re, time, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "# from nltk.tag.simplify import simplify_wsj_tag\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk import load_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9723, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_feather('03-post.feather')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Copy Some Code\n",
    "http://datadesk.latimes.com/posts/2013/12/natural-language-processing-in-the-kitchen/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training = {\n",
    "    'units': [\n",
    "        'five ISIS well heads',\n",
    "        'an oil equipment piece',\n",
    "        'a tactical vehicle',\n",
    "        'a VBIED',\n",
    "        \"three VBIED factories\",\n",
    "        \"two ISIS-held buildings\"\n",
    "    ],\n",
    "    'locations': [\n",
    "        'near raqqa',\n",
    "        'near mosul'\n",
    "    ],\n",
    "    'verbs': [\n",
    "        'one strike engaged',\n",
    "        'two strikes destroyed',\n",
    "        'four strikes destroyed',\n",
    "        'and damaged',\n",
    "        '13 strikes engaged',\n",
    "        'six strikes engaged',\n",
    "        'and destroyed'\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.467\n",
      "             2          -0.53080        1.000\n",
      "             3          -0.35423        1.000\n",
      "             4          -0.26650        1.000\n",
      "             5          -0.21380        1.000\n",
      "             6          -0.17858        1.000\n",
      "             7          -0.15335        1.000\n",
      "             8          -0.13439        1.000\n",
      "             9          -0.11961        1.000\n",
      "            10          -0.10776        1.000\n",
      "            11          -0.09805        1.000\n",
      "            12          -0.08995        1.000\n",
      "            13          -0.08309        1.000\n",
      "            14          -0.07720        1.000\n",
      "            15          -0.07209        1.000\n",
      "            16          -0.06762        1.000\n",
      "            17          -0.06367        1.000\n",
      "            18          -0.06016        1.000\n",
      "            19          -0.05701        1.000\n",
      "            20          -0.05418        1.000\n",
      "            21          -0.05161        1.000\n",
      "            22          -0.04928        1.000\n",
      "            23          -0.04715        1.000\n",
      "            24          -0.04520        1.000\n",
      "            25          -0.04340        1.000\n",
      "            26          -0.04174        1.000\n",
      "            27          -0.04020        1.000\n",
      "            28          -0.03877        1.000\n",
      "            29          -0.03744        1.000\n",
      "            30          -0.03620        1.000\n",
      "            31          -0.03504        1.000\n",
      "            32          -0.03395        1.000\n",
      "            33          -0.03292        1.000\n",
      "            34          -0.03196        1.000\n",
      "            35          -0.03105        1.000\n",
      "            36          -0.03019        1.000\n",
      "            37          -0.02938        1.000\n",
      "            38          -0.02861        1.000\n",
      "            39          -0.02788        1.000\n",
      "            40          -0.02718        1.000\n",
      "            41          -0.02652        1.000\n",
      "            42          -0.02589        1.000\n",
      "            43          -0.02529        1.000\n",
      "            44          -0.02472        1.000\n",
      "            45          -0.02417        1.000\n",
      "            46          -0.02365        1.000\n",
      "            47          -0.02315        1.000\n",
      "            48          -0.02267        1.000\n",
      "            49          -0.02221        1.000\n",
      "            50          -0.02176        1.000\n",
      "            51          -0.02134        1.000\n",
      "            52          -0.02093        1.000\n",
      "            53          -0.02054        1.000\n",
      "            54          -0.02016        1.000\n",
      "            55          -0.01979        1.000\n",
      "            56          -0.01944        1.000\n",
      "            57          -0.01910        1.000\n",
      "            58          -0.01877        1.000\n",
      "            59          -0.01845        1.000\n",
      "            60          -0.01815        1.000\n",
      "            61          -0.01785        1.000\n",
      "            62          -0.01756        1.000\n",
      "            63          -0.01729        1.000\n",
      "            64          -0.01702        1.000\n",
      "            65          -0.01676        1.000\n",
      "            66          -0.01650        1.000\n",
      "            67          -0.01626        1.000\n",
      "            68          -0.01602        1.000\n",
      "            69          -0.01579        1.000\n",
      "            70          -0.01556        1.000\n",
      "            71          -0.01534        1.000\n",
      "            72          -0.01513        1.000\n",
      "            73          -0.01492        1.000\n",
      "            74          -0.01472        1.000\n",
      "            75          -0.01453        1.000\n",
      "            76          -0.01434        1.000\n",
      "            77          -0.01415        1.000\n",
      "            78          -0.01397        1.000\n",
      "            79          -0.01379        1.000\n",
      "            80          -0.01362        1.000\n",
      "            81          -0.01345        1.000\n",
      "            82          -0.01329        1.000\n",
      "            83          -0.01313        1.000\n",
      "            84          -0.01298        1.000\n",
      "            85          -0.01282        1.000\n",
      "            86          -0.01267        1.000\n",
      "            87          -0.01253        1.000\n",
      "            88          -0.01239        1.000\n",
      "            89          -0.01225        1.000\n",
      "            90          -0.01211        1.000\n",
      "            91          -0.01198        1.000\n",
      "            92          -0.01185        1.000\n",
      "            93          -0.01172        1.000\n",
      "            94          -0.01160        1.000\n",
      "            95          -0.01148        1.000\n",
      "            96          -0.01136        1.000\n",
      "            97          -0.01124        1.000\n",
      "            98          -0.01113        1.000\n",
      "            99          -0.01101        1.000\n",
      "         Final          -0.01090        1.000\n"
     ]
    }
   ],
   "source": [
    "# Set up a list that will contain all of our tagged examples,\n",
    "# which we will pass into the classifier at the end.\n",
    "training_set = []\n",
    "for key, val in training.items():\n",
    "    for i in val:\n",
    "        # Set up a list we can use for all of our features,\n",
    "        # which are just individual words in this case.\n",
    "        feats = []\n",
    "        # Before we can tokenize words, we need to break the\n",
    "        # text out into sentences.\n",
    "        sentences = nltk.sent_tokenize(i)\n",
    "        for sentence in sentences:\n",
    "            feats = feats + nltk.word_tokenize(sentence)\n",
    "\n",
    "        # For this example, it's a good idea to normalize for case.\n",
    "        # You may or may not need to do this.\n",
    "        feats = [i.lower() for i in feats]\n",
    "        # Each feature needs a value. A typical use for a case like this\n",
    "        # is to use True or 1, though you can use almost any value for\n",
    "        # a more complicated application or analysis.\n",
    "        feats = dict([(i, True) for i in feats])\n",
    "        # NLTK expects you to feed a classifier a list of tuples\n",
    "        # where each tuple is (features, tag).\n",
    "        training_set.append((feats, key))\n",
    "\n",
    "# Train up our classifier\n",
    "classifier = MaxentClassifier.train(training_set)\n",
    "\n",
    "# Test it out!\n",
    "# You need to feed the classifier your data in the same format you used\n",
    "# to train it, in this case individual lowercase words.\n",
    "classifier.classify({'VBIED': True, 'ISIS-held buildings': True})\n",
    "\n",
    "# Save it to disk, if you want, because these can take a long time to train.\n",
    "outfile = open('classifier.pickle', 'wb')\n",
    "pickle.dump(classifier, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'near': True,\n",
       " 'tabqah': True,\n",
       " ',': True,\n",
       " '13': True,\n",
       " 'strikes': True,\n",
       " 'engaged': True,\n",
       " 'nine': True,\n",
       " 'isis': True,\n",
       " 'tactical': True,\n",
       " 'units': True,\n",
       " ';': True,\n",
       " 'destroyed': True,\n",
       " 'two': True,\n",
       " 'fighting': True,\n",
       " 'positions': True,\n",
       " '.': True,\n",
       " 'ADP': True,\n",
       " 'NOUN': True,\n",
       " 'NUM': True,\n",
       " 'VERB': True,\n",
       " 'ADJ': True,\n",
       " 'near/tabqah/,': True,\n",
       " 'tabqah/,/13': True,\n",
       " ',/13/strikes': True,\n",
       " '13/strikes/engaged': True,\n",
       " 'strikes/engaged/nine': True,\n",
       " 'engaged/nine/isis': True,\n",
       " 'nine/isis/tactical': True,\n",
       " 'isis/tactical/units': True,\n",
       " 'tactical/units/;': True,\n",
       " 'units/;/destroyed': True,\n",
       " ';/destroyed/two': True,\n",
       " 'destroyed/two/fighting': True,\n",
       " 'two/fighting/positions': True,\n",
       " 'fighting/positions/.': True}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "# posTagged = pos_tag(text)\n",
    "# simplifiedTags = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in posTagged]\n",
    "# print(simplifiedTags)\n",
    "\n",
    "def get_features(text):\n",
    "    words = []\n",
    "    # Same steps to start as before\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = words + nltk.word_tokenize(sentence)\n",
    "\n",
    "    # part of speech tag each of the words\n",
    "    pos = nltk.pos_tag(words)\n",
    "    # Sometimes it's helpful to simplify the tags NLTK returns by default.\n",
    "    # I saw an increase in accuracy if I did this, but you may not\n",
    "    # depending on the application.\n",
    "    pos = [map_tag('en-ptb', 'universal',tag) for word, tag in pos]\n",
    "    # Then, convert the words to lowercase like before\n",
    "    words = [i.lower() for i in words]\n",
    "    # Grab the trigrams\n",
    "    trigrams = nltk.trigrams(words)\n",
    "    # We need to concatinate the trigrams into a single string to process\n",
    "    trigrams = [\"%s/%s/%s\" % (i[0], i[1], i[2]) for i in trigrams]\n",
    "    # Get our final dict rolling\n",
    "    features = words + pos + trigrams\n",
    "    # get our feature dict rolling\n",
    "    features = dict([(i, True) for i in features])\n",
    "    return features\n",
    "\n",
    "# Try it out\n",
    "text = \"Near Tabqah, 13 strikes engaged nine ISIS tactical units; destroyed two fighting positions.\"\n",
    "get_features(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'detroit,', 'strikes', 'destroyed'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-14165080ff48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'near detroit, two strikes destroyed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file:atis.cfg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SEM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             raise ValueError(\n\u001b[0;32m--> 684\u001b[0;31m                 \u001b[0;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[0;34m\"input words: %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m             )\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'detroit,', 'strikes', 'destroyed'\"."
     ]
    }
   ],
   "source": [
    "query = 'near detroit, two strikes destroyed'\n",
    "cp = load_parser('file:atis.cfg')\n",
    "trees = list(cp.parse(query.split()))\n",
    "answer = trees[0].label()['SEM']\n",
    "answer = [s for s in answer if s]\n",
    "q = ' '.join(answer)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near Dayr Az Zawr, four strikes destroyed five ISIS well heads, two pump jacks, an oil storage tank, and an oil equipment piece\n",
      "[('Near', 'IN'), ('Dayr', 'NNP'), ('Az', 'NNP'), ('Zawr', 'NNP'), (',', ','), ('four', 'CD'), ('strikes', 'NNS'), ('destroyed', 'VBN'), ('five', 'CD'), ('ISIS', 'NNP'), ('well', 'RB'), ('heads', 'NNS'), (',', ','), ('two', 'CD'), ('pump', 'NN'), ('jacks', 'NNS'), (',', ','), ('an', 'DT'), ('oil', 'NN'), ('storage', 'NN'), ('tank', 'NN'), (',', ','), ('and', 'CC'), ('an', 'DT'), ('oil', 'NN'), ('equipment', 'NN'), ('piece', 'NN')]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'N'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ce4b701cf94e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtagged_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpret_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrammar_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msynrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemrep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/sem/util.py\u001b[0m in \u001b[0;36minterpret_sents\u001b[0;34m(inputs, grammar, semkey, trace)\u001b[0m\n\u001b[1;32m     86\u001b[0m     return [\n\u001b[1;32m     87\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_semrep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyntrees\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msyntrees\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     ]\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/sem/util.py\u001b[0m in \u001b[0;36mparse_sents\u001b[0;34m(inputs, grammar, trace)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# use a tokenizer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msyntrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mparses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyntrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             raise ValueError(\n\u001b[0;32m--> 684\u001b[0;31m                 \u001b[0;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[0;34m\"input words: %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m             )\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'N'\"."
     ]
    }
   ],
   "source": [
    "# https://github.com/readywater/text-analytics-w-python-2e/blob/master/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb\n",
    "\n",
    "sents = 'Near Dayr Az Zawr, four strikes destroyed five ISIS well heads, two pump jacks, an oil storage tank, and an oil equipment piece'\n",
    "grammar_file = 'atis.cfg'\n",
    "print(sents)\n",
    "\n",
    "from nltk.tag import RegexpTagger \n",
    "# define regex tag patterns \n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'), # gerunds         \n",
    "    (r'.*ed$', 'VBD'), # simple past         \n",
    "    (r'.*es$', 'VBZ'), # 3rd singular present         \n",
    "    (r'.*ould$', 'MD'), # modals         \n",
    "    (r'.*\\'s$', 'NN$'), # possessive nouns         \n",
    "    (r'.*s$', 'NNS'), # plural nouns         \n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers         \n",
    "    (r'.*', 'NN') # nouns (default) ... \n",
    "] \n",
    "rt=RegexpTagger(patterns)\n",
    "\n",
    "tagged_sent = nltk.pos_tag(nltk.word_tokenize(sents))\n",
    "print(tagged_sent)\n",
    "for results in nltk.interpret_sents(sents, grammar_file):\n",
    "    for (synrep, semrep) in results:\n",
    "        print(synrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'Nera Dayr Az Zawr, four strikes destroyed five ISIS well heads, two pump jacks, an oil storage tank, and an oil equipment piece.', 'Near Tabqah, 13 strikes engaged nine ISIS tactical units; destroyed two fighting positions, two vehicles, a tactical vehicle, and a tunnel; and damaged two supply routes.', 'Near Al Qaim, one strike destroyed two anti-air artillery systems and two ISIS heldbuildings.', 'Near Haditha, one strike engaged an ISIS tactical unit; and destroyed a VBIED and a vehicle.', 'Near Mosul, six strikes engaged four ISIS tactical units; destroyed five mortar systems, three VBIED factories, three ISIS-held buildings, two anti-air artillery systems, two supply caches, a tactical vehicle, a vehicle, and a weapons facility; damaged nine supply routes, and suppressed six ISIS mortar teams.'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1a49f576bf1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'Near Haditha, one strike engaged an ISIS tactical unit; and destroyed a VBIED and a vehicle.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         'Near Mosul, six strikes engaged four ISIS tactical units; destroyed five mortar systems, three VBIED factories, three ISIS-held buildings, two anti-air artillery systems, two supply caches, a tactical vehicle, a vehicle, and a weapons facility; damaged nine supply routes, and suppressed six ISIS mortar teams.',]\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             raise ValueError(\n\u001b[0;32m--> 684\u001b[0;31m                 \u001b[0;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[0;34m\"input words: %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m             )\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'Nera Dayr Az Zawr, four strikes destroyed five ISIS well heads, two pump jacks, an oil storage tank, and an oil equipment piece.', 'Near Tabqah, 13 strikes engaged nine ISIS tactical units; destroyed two fighting positions, two vehicles, a tactical vehicle, and a tunnel; and damaged two supply routes.', 'Near Al Qaim, one strike destroyed two anti-air artillery systems and two ISIS heldbuildings.', 'Near Haditha, one strike engaged an ISIS tactical unit; and destroyed a VBIED and a vehicle.', 'Near Mosul, six strikes engaged four ISIS tactical units; destroyed five mortar systems, three VBIED factories, three ISIS-held buildings, two anti-air artillery systems, two supply caches, a tactical vehicle, a vehicle, and a weapons facility; damaged nine supply routes, and suppressed six ISIS mortar teams.'\"."
     ]
    }
   ],
   "source": [
    "\n",
    "original_grammar = nltk.data.load('atis.cfg')\n",
    "original_parser = nltk.ChartParser(original_grammar)\n",
    "sent = ['Nera Dayr Az Zawr, four strikes destroyed five ISIS well heads, two pump jacks, an oil storage tank, and an oil equipment piece.', \n",
    "         'Near Tabqah, 13 strikes engaged nine ISIS tactical units; destroyed two fighting positions, two vehicles, a tactical vehicle, and a tunnel; and damaged two supply routes.',\n",
    "        'Near Al Qaim, one strike destroyed two anti-air artillery systems and two ISIS heldbuildings.',\n",
    "        'Near Haditha, one strike engaged an ISIS tactical unit; and destroyed a VBIED and a vehicle.',\n",
    "        'Near Mosul, six strikes engaged four ISIS tactical units; destroyed five mortar systems, three VBIED factories, three ISIS-held buildings, two anti-air artillery systems, two supply caches, a tactical vehicle, a vehicle, and a weapons facility; damaged nine supply routes, and suppressed six ISIS mortar teams.',]\n",
    "for i in original_parser.parse(sent):\n",
    "    print(i)\n",
    "    break\n",
    "# original_grammar._rhs_index['mosul']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
